{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "372ff1cd",
   "metadata": {},
   "source": [
    "# CycleGAN — Bug-Fix Labs - 10 Bugs to Fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60f1544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import itertools\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ---------------------------\n",
    "# Data  (Unpaired domains X, Y built from CIFAR-10 and CIFAR-10 with color jitter)\n",
    "# ---------------------------\n",
    "tf_X = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.ToTensor()                             # BUG\n",
    "])\n",
    "tf_Y = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "    transforms.ToTensor()                             # BUG\n",
    "])\n",
    "\n",
    "class UnpairedCIFAR(Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        self.X = torchvision.datasets.CIFAR10('./data', train=train, download=True, transform=tf_X)\n",
    "        self.Y = torchvision.datasets.CIFAR10('./data', train=train, download=True, transform=tf_Y)\n",
    "    def __len__(self): return min(len(self.X), len(self.Y))\n",
    "    def __getitem__(self, i):\n",
    "        x,_ = self.X[i]\n",
    "        y,_ = self.Y[(i*7) % len(self.Y)]   # different index ⇒ unpaired\n",
    "        return x, y\n",
    "\n",
    "ds = UnpairedCIFAR(train=True)\n",
    "loader = DataLoader(ds, batch_size=4, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n",
    "\n",
    "# ---------------------------\n",
    "# Models\n",
    "# ---------------------------\n",
    "def conv(in_c, out_c, k, s, p, norm=True):\n",
    "    layers = [nn.Conv2d(in_c, out_c, k, s, p)]\n",
    "    # BUG\n",
    "    if norm: layers += [nn.BatchNorm2d(out_c)]  # WRONG: use InstanceNorm2d\n",
    "    layers += [nn.ReLU(True)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, ch):\n",
    "        super().__init__()\n",
    "        self.c1 = conv(ch, ch, 3, 1, 1)   # uses BN (bug above)\n",
    "        self.c2 = conv(ch, ch, 3, 1, 1)   # uses BN (bug above)\n",
    "    def forward(self, x): return x + self.c2(self.c1(x))\n",
    "\n",
    "class ResGen(nn.Module):\n",
    "    def __init__(self, in_ch=3, ch=64, n_blocks=3):\n",
    "        super().__init__()\n",
    "        self.down = nn.Sequential(\n",
    "            conv(in_ch, ch,   7, 1, 3),\n",
    "            conv(ch,   ch*2,  3, 2, 1),\n",
    "            conv(ch*2, ch*4,  3, 2, 1),\n",
    "        )\n",
    "        self.res = nn.Sequential(*[ResBlock(ch*4) for _ in range(n_blocks)])\n",
    "        self.up  = nn.Sequential(\n",
    "            nn.ConvTranspose2d(ch*4, ch*2, 3, 2, 1, output_padding=1), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ch*2, ch,   3, 2, 1, output_padding=1), nn.ReLU(True),\n",
    "            nn.Conv2d(ch, 3, 7, 1, 3)   # BUG\n",
    "                                        # BUG\n",
    "        )\n",
    "    def forward(self, x): return self.up(self.res(self.down(x)))\n",
    "\n",
    "class PatchD(nn.Module):\n",
    "    def __init__(self, in_ch=3, ch=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, ch, 4, 2, 1), nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(ch, ch*2, 4, 2, 1), nn.BatchNorm2d(ch*2), nn.LeakyReLU(0.2, True),  # BUG\n",
    "            nn.Conv2d(ch*2, ch*4, 4, 2, 1), nn.BatchNorm2d(ch*4), nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(ch*4, 1, 1, 1, 0),\n",
    "            nn.Sigmoid()   # BUG\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)  # (B,1,H,W)\n",
    "\n",
    "# BUG\n",
    "D_shared = PatchD().to(device)\n",
    "\n",
    "G_XY = ResGen().to(device)   # X → Y\n",
    "G_YX = ResGen().to(device)   # Y → X\n",
    "\n",
    "# ---------------------------\n",
    "# Losses & Optimizers\n",
    "# ---------------------------\n",
    "bce = nn.BCEWithLogitsLoss()\n",
    "l1  = nn.L1Loss()\n",
    "\n",
    "# BUG\n",
    "optD = torch.optim.Adam(D_shared.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "optG = torch.optim.Adam(itertools.chain(G_XY.parameters(), G_YX.parameters()), lr=2e-4, betas=(0.5, 0.999))\n",
    "\n",
    "lambda_cyc = 10.0\n",
    "\n",
    "def cycle_consistency_loss(G_XY, G_YX, x, y, lam=10.0):\n",
    "    # BUG\n",
    "    cyc_x = G_YX(x)                    # WRONG\n",
    "    loss = lam * F.l1_loss(cyc_x, x)   # WRONG\n",
    "    return loss\n",
    "\n",
    "# ---------------------------\n",
    "# Training loop (intentionally wrong)\n",
    "# ---------------------------\n",
    "for x, y in loader:\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    # ---- Discriminator steps ----\n",
    "    # BUG\n",
    "    fake_y = G_XY(x)                                 # BUG\n",
    "    fake_x = G_YX(y)                                 # BUG\n",
    "    optD.zero_grad()\n",
    "\n",
    "    logits_real_X = D_shared(x)\n",
    "    logits_fake_X = D_shared(fake_x)                 # same D judging X-domain fakes\n",
    "    # BUG\n",
    "    t1 = torch.ones(x.size(0), 1, device=device)     # WRONG\n",
    "    t0 = torch.zeros(x.size(0), 1, device=device)    # WRONG\n",
    "    lossDX = bce(logits_real_X, t1) + bce(logits_fake_X, t0)\n",
    "\n",
    "    logits_real_Y = D_shared(y)\n",
    "    logits_fake_Y = D_shared(fake_y)\n",
    "    lossDY = bce(logits_real_Y, t1) + bce(logits_fake_Y, t0)\n",
    "\n",
    "    lossD = (lossDX + lossDY) * 0.5\n",
    "    lossD.backward()\n",
    "    optG.step()                                      # BUG\n",
    "\n",
    "    # ---- Generator step ----\n",
    "    # BUG\n",
    "    optG.zero_grad()\n",
    "    adv_X = bce(D_shared(G_YX(y)), torch.ones_like(logits_real_X))   # judge Y→X\n",
    "    adv_Y = bce(D_shared(G_XY(x)), torch.zeros_like(logits_real_Y))  # BUG\n",
    "    # BUG\n",
    "    loss_cyc = cycle_consistency_loss(G_XY, G_YX, x, y, lambda_cyc)  # BUG\n",
    "    lossG = adv_X + adv_Y + loss_cyc\n",
    "    lossG.backward()\n",
    "    # (missing)                            # BUG\n",
    "    break  # keep the broken demo short\n",
    "\n",
    "print(\"Your task: fix all bugs until CycleGAN training runs and X↔Y translations look reasonable.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
